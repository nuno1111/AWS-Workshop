{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725f25fc-77e9-4fb1-a88c-1b5ebe9c5a12",
   "metadata": {},
   "source": [
    "# 🚀 SageMaker Lakehouse: 판매 및 프로모션 데이터 연결을 위한 ML 솔루션\n",
    "\n",
    "## 📊 개요\n",
    "\n",
    "이 노트북은 Amazon SageMaker Lakehouse를 활용하여 서로 다른 저장 시스템에서 가져온 판매 및 프로모션 데이터를 통합하는 통합 데이터셋을 생성하는 방법을 보여줍니다. 이렇게 생성된 테이블은 머신 러닝 모델 개발의 기반이 될 것입니다.\n",
    "\n",
    "## 🛠️ 기술 환경                                                                                                                                                                    \n",
    "우리는 다음과 같은 기능을 제공하는 Amazon SageMaker Lakehouse를 활용합니다:\n",
    "\n",
    "- 🔄 Amazon S3 데이터 레이크와 Amazon Redshift 데이터 웨어하우스 전반에 걸친 통합 데이터 액세스\n",
    "- 🔍 Apache Iceberg 호환 도구를 사용한 원활한 쿼리 기능\n",
    "- 💎 데이터 복제 없음: 데이터가 있는 곳에서 직접 쿼리하여 복사본 생성 필요성 제거\n",
    "\n",
    "\n",
    "## 🏢 비즈니스 맥락\n",
    "\n",
    "분석 팀의 구성원으로서, 우리는 판매 성과와 프로모션 활동을 연관시키는 데이터셋을 준비해야 합니다. 우리의 데이터 소스는 두 부서에 걸쳐 있습니다:\n",
    "\n",
    "- 중앙 운영 📦: Amazon S3 데이터 레이크에 저장된 판매 데이터\n",
    "- 백오피스 💼: Amazon Redshift에서 관리되는 프로모션 데이터\n",
    "\n",
    "## 🎯 목표\n",
    "\n",
    "우리는 다음과 같은 새로운 분석 테이블을 생성할 것입니다:\n",
    "\n",
    "1. 🔗 S3의 판매 거래 기록과 Redshift의 프로모션 데이터 통합\n",
    "2. 📈 각 판매 시점에 제품 카테고리 및 지역별 활성 프로모션 수 계산\n",
    "3. 🤖 머신 러닝 팀과 공유할 준비가 된 정제되고 최적화된 데이터셋 제공\n",
    "    \n",
    "\n",
    "💡 SageMaker Lakehouse의 통합 쿼리 인터페이스는 여러 저장 솔루션 작업의 복잡성을 제거하여, 데이터 액세스 메커니즘보다는 분석 요구사항에 집중할 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a28f5-3827-4c8b-8a8c-dcbc5e57cf89",
   "metadata": {},
   "source": [
    "## 🔧 설정 및 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb36ba-e2d0-40e3-9e0d-b59aed14c847",
   "metadata": {},
   "source": [
    "필요한 라이브러리와 Spark 세션 구성으로 개발 환경을 초기화해 보겠습니다. 다음 설정은 Amazon S3에 저장된 데이터 레이크 테이블과 Amazon Redshift의 데이터 웨어하우스 테이블의 원활한 통합을 가능하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ccca3b-aee2-42a4-81db-ac3b00822a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# 현재 AWS 계정 ID를 검색하는 함수\n",
    "def get_account_id():\n",
    "    sts = boto3.client('sts')\n",
    "    return sts.get_caller_identity()['Account']\n",
    "\n",
    "# 현재 AWS 계정 ID 가져오기\n",
    "account_id = get_account_id()\n",
    "\n",
    "# 다음을 포함한 Spark 세션 구성 딕셔너리 정의:\n",
    "# - Iceberg 테이블 형식 지원\n",
    "# - Spark SQL 확장 및 카탈로그 설정\n",
    "\n",
    "config_dict = {\n",
    "    \"--datalake-formats\": \"iceberg\",\n",
    "    \"--conf\": f\"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.rms_federated_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.rms_federated_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.rms_federated_catalog.glue.id={account_id}:federated_redshift_catalog/enterprise_operations --conf spark.sql.catalog.rms_federated_catalog.client.region=us-east-1 --conf spark.sql.catalog.rms_federated_catalog.glue.account-id={account_id} --conf spark.sql.catalog.spark_catalog.client.region=us-east-1 --conf spark.sql.catalog.spark_catalog.glue.account-id={account_id}\"\n",
    "}\n",
    "\n",
    "# 구성을 적절히 형식이 지정된 JSON 문자열로 변환\n",
    "# JSON 호환성을 위해 작은따옴표를 큰따옴표로 대체\n",
    "config_json = json.dumps(config_dict, indent=4).replace(\"'\", '\"')\n",
    "\n",
    "# IPython 인터페이스를 가져와서 구성 설정\n",
    "ip = get_ipython()\n",
    "ip.run_cell_magic('configure', '-f --name project.spark.fineGrained', config_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afe34b-092d-40a7-b76d-e3d3d94163d0",
   "metadata": {},
   "source": [
    "💡 참고: 다음 셀을 실행하면 Glue 인터랙티브 세션이 초기화되며 완료하는 데 약 1분 정도 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae0925-9744-4824-bb76-332676f25362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b185d-f2f1-4e0e-bebf-a23952922e3e",
   "metadata": {},
   "source": [
    "## 📊 데이터 탐색: Glue 데이터 카탈로그 (S3 데이터 레이크)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371e7a6-e9a0-4861-932f-29641cdc0787",
   "metadata": {},
   "source": [
    "S3 기반 데이터 레이크에서 Glue 기본 카탈로그를 통해 사용 가능한 데이터를 살펴보겠습니다.\n",
    "\n",
    "기본 Glue 카탈로그에서 사용 가능한 모든 데이터베이스 나열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d9b1c-5673-4e2a-a2db-585b05516eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show databases\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f95d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "\n",
    "# 프로젝트 데이터베이스 이름 가져오기\n",
    "project_db = spark.sql(\"show databases\").collect()[0]['namespace']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84581f29-d3a6-45d5-ab3f-a269cdbd5790",
   "metadata": {},
   "source": [
    "기본 데이터베이스의 테이블 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411c520-bddb-4a58-a875-f40c7c2476e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show tables from {project_db}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289ee87-2d33-474f-aa92-0a8f41d30af9",
   "metadata": {},
   "source": [
    "`sales_data` 테이블 데이터 보기 (처음 10개 레코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a9193-dd0e-4473-9a7c-9c7b0e477582",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"select * from {project_db}.sales_data limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fabb5b-37d2-41d5-ad87-586ffc50f45e",
   "metadata": {},
   "source": [
    "## 📊 데이터 탐색: Redshift 연합 카탈로그"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14194649-8d0e-4b5c-9c01-3d27a09d92e3",
   "metadata": {},
   "source": [
    "이제 Redshift 데이터 웨어하우스에 저장된 데이터를 살펴보겠습니다.\n",
    "\n",
    "Redshift에서 사용 가능한 모든 데이터베이스 나열."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00d199-520e-452c-9797-714f83ec1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show databases in rms_federated_catalog\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2c9b1-db23-4286-be78-3f875cea4314",
   "metadata": {},
   "source": [
    "Redshift의 `public` 스키마에 있는 테이블 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46513372-f222-406e-9ad3-60e3e9f6aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"show tables from rms_federated_catalog.public\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fe14f-ac6d-4611-a50b-037e068f6ba4",
   "metadata": {},
   "source": [
    "`promotions` 테이블 데이터 보기 (처음 10개 레코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3a8a4-7709-4b64-b124-9e1797aa5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "# Redshift 연결 시 콜드 스타트 문제 처리 Redshift Serverless 의 경우 장시간 사용하지 않은 경우 시작할 때 시간이 걸리는 경우가 있어, 간단한 쿼리로 DB를 깨우는 작업을 합니다\n",
    "try:\n",
    "    spark.sql(f\"select * from rms_federated_catalog.public.promotions limit 1\").count()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8248c-e267-4d3f-98d3-502b98a8703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"select * from rms_federated_catalog.public.promotions limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f5b6d-42e0-4f5d-a3b3-e901cc60f61b",
   "metadata": {},
   "source": [
    "## 🔄 ML 모델링을 위한 판매 및 프로모션 데이터셋 준비\n",
    "\n",
    "이 섹션에서는 데이터 레이크의 판매 데이터와 데이터 웨어하우스의 프로모션 정보를 결합하여 향후 머신 러닝 모델링을 위한 특성이 풍부한 데이터셋을 만듭니다. 결과 테이블에는 각 판매 제품 카테고리 및 지역에 대한 활성 프로모션 수가 포함됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eef3d3-b702-4d1b-b2a3-e4d32c8753c7",
   "metadata": {},
   "source": [
    "보강된 데이터셋을 프로젝트 데이터베이스의 새 테이블로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b7e73-1700-4ecf-b1c1-709b7b5391cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "\n",
    "# 판매 데이터와 프로모션을 조인하고 판매당 활성 프로모션 계산\n",
    "final_table = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    s.*,\n",
    "    COUNT(p.promotion_id) as active_promotions\n",
    "FROM \n",
    "    {project_db}.sales_data s\n",
    "LEFT JOIN \n",
    "    rms_federated_catalog.public.promotions p \n",
    "    ON s.region = p.region\n",
    "    AND s.product_category = p.product_category\n",
    "    AND s.order_date BETWEEN p.start_date AND p.end_date\n",
    "GROUP BY \n",
    "    s.region,\n",
    "    s.country,\n",
    "    s.item_type,\n",
    "    s.product_category,\n",
    "    s.sales_channel,\n",
    "    s.order_priority,\n",
    "    s.order_date,\n",
    "    s.order_id,\n",
    "    s.ship_date,\n",
    "    s.units_sold,\n",
    "    s.unit_price,\n",
    "    s.unit_cost,\n",
    "    s.total_revenue,\n",
    "    s.total_cost,\n",
    "    s.total_profit\n",
    "\"\"\")\n",
    "\n",
    "# 최종 데이터셋의 임시 뷰 생성\n",
    "final_table.createOrReplaceTempView(\"temp_final_table\")\n",
    "\n",
    "# CTAS(Create Table As Select)를 사용하여 영구 테이블 생성\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {project_db}.sales_table_enriched_w_campaigns\n",
    "    USING PARQUET\n",
    "    AS \n",
    "    SELECT * FROM temp_final_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d09d4-27e6-4a89-8bff-94fb0ea5b1e8",
   "metadata": {},
   "source": [
    "새로 생성된 보강 테이블 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1d33f-10af-4960-a6a0-17cb1cb372f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark.fineGrained\n",
    "spark.sql(f\"select * from {project_db}.sales_table_enriched_w_campaigns limit 10\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
