{
	"metadata": {
		"pycharm": {
			"stem_cell": {
				"cell_type": "raw",
				"source": [],
				"metadata": {
					"collapsed": false
				}
			}
		},
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Redshift Lab\n\nTake your time to read through the instructions provided in this notebook.\n\n#### Learning Objectives\n\n- Understand how to interactivly author Glue ETL scripts using Glue Studio & Jupyter notebooks (This portion has already been covered under \"Transform Data with AWS Glue interactive sessions\" module). \n- Use Glue to do record level transformations and write them to redshift tables. \n\n**Note:** \n  - **Execute the code blocks one cell at a time.**\n  - **It's a good practice to keep saving the notebook at regular intervals while you work through it.** Read more about saving the notebook here: https://docs.aws.amazon.com/glue/latest/ug/notebook-getting-started.html#save-notebook",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# Initial configuration\n- Lets configure \"session idle timeout\", \"worker type\", \"number of workers\" and \"redshift connection\" with the help of available magics. \n  - **%idle_timeout**: The number of minutes of inactivity after which a session will timeout after a cell has been executed. \n  - **%worker_type**: Type of workers supported by AWS Glue. Default is G.1X.\n  - **%number_of_workers**: The number of workers of a defined worker_type that are allocated when a job runs.\n  - **%connections**: Specify a comma-separated list of connections to use in the session.\n\n#### Read more about magics supported by AWS Glue interactive sessions for Jupyter here: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions-magics.html  \n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%worker_type G.1X\n%number_of_workers 2\n%connections analytics_workshop",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Import Libraries \n- In this notebook we will be using the following classes, here are some of the important ones\n    - SparkContext - Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.\n    - GlueContext - Wraps the Apache SparkSQL SQLContext object, and thereby provides mechanisms for interacting with the Apache Spark platform\n    - boto3 - AWS's Python SDK, we will be using this library to make call to AWS APIs.\n    - awsglue - AWS's pyspark library that provides the needed Python packages and extends PySpark to support serverless ETL on AWS.\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Exploring your raw dataset\n- In this step you will:\n    - Create a dynamic frame for your 'raw' table from AWS Glue catalog\n    - Explore the schema of the datasets\n    - Count rows in raw table\n    - View a sample of the data \n\n### Glue Dynamic Frames Basics\n\n- AWS Glue's dynamic data frames is a powerful data structure.\n- They provide a precise representation of the underlying semi-structured data, especially when dealing with columns or fields with varying types.\n- They also provide powerful primitives to deal with nesting and unnesting.\n- A dynamic record is a self-describing record: Each record encodes its columns and types, so every record can have a schema that is unique from all others in the dynamic frame.\n- For ETL, we needed somthing more dynamic, hence we created the Glue Dynamic DataFrames. DDF are an implementaion of DF that relaxes the requiements of having a rigid schema. Its designed for semi-structured data.\n- It maintains a schema per-record, its easy to restucture, tag and modify. \n\n\n#### Read More : https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "glueContext = GlueContext(SparkContext.getOrCreate())\nspark = glueContext.spark_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Crate dynamic frame from Glue catalog\n- In this block we are using gluecontext to create a new dynamicframe from glue catalog\n\nOther ways to create dynamicframes in Glue:\n- create_dynamic_frame_from_rdd\n- create_dynamic_frame_from_catalog\n- create_dynamic_frame_from_options\n\n#### Read More:https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data = glueContext.create_dynamic_frame.from_catalog(database=\"analyticsworkshopdb\", table_name=\"raw\")\n\nreference_data = glueContext.create_dynamic_frame.from_catalog(database=\"analyticsworkshopdb\", table_name=\"reference_data\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## View schema\n- In this step we view the schema of the dynamic frame\n- printSchema(): Prints the schema of the underlying DataFrame.\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "reference_data.printSchema()",
			"metadata": {
				"scrolled": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Count records\n- In this step we will count the number of records in the dataframe\n- count(): Returns the number of rows in the underlying DataFrame\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "print(f'raw_data (count) = {raw_data.count()}')\nprint(f'reference_data (count) = {reference_data.count()}')",
			"metadata": {
				"scrolled": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Show sample raw records\n- to.DF()method converts a DynamicFrame to an Apache Spark DataFrame by converting DynamicRecords into DataFrame fields\n- use show() method to display a sample of records in the frame\n- here were are showing the top 5 records in the DF\n\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data.toDF().show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Define Transformation Functions\n- You can define attribute level transformation functions (**load_time_fn** here). \"**load_time_fn**\" combines partition column values into one single attribute \"**load_time**\" in YYYYMMDDHH24 format as an integer. \n- Call all attribute level transformation functions for each record in dynamic dataframe in record level transformation function (**transformRec** here)\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def load_time_fn(partition_0, partition_1, partition_2, partition_3):\n    x = partition_0 + partition_1 + partition_2 + partition_3\n    x = int(x)\n    return x",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "def transformRec(rec):\n    rec[\"load_time\"] = load_time_fn(rec[\"partition_0\"], rec[\"partition_1\"], rec[\"partition_2\"], rec[\"partition_3\"])\n    return rec",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Apply Transformations\n- Apply all transformations and store it back in dynamic data frame - \"**raw_data_x**\"\n\n###### Read more about AWS Glue transforms here : https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data_x = Map.apply(frame=raw_data, f=transformRec)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Show sample transformed raw records\n- to.DF()method converts a DynamicFrame to an Apache Spark DataFrame by converting DynamicRecords into DataFrame fields\n- use show() method to display a sample of records in the frame\n- here were are showing the top 5 records in the DF\n\n#### Read more about AWS Glue transforms here : https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data_x.toDF().show(5)",
			"metadata": {
				"scrolled": true,
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Drop fields\n- Once \"**load_time**\" attribute is generated, we will drop original partition columns using \"**drop_fields**\" method.\n- These were generated by firehose for placing the files in yyyy/mm/dd/hh directory structure in S3\n- We will use Glue's in-built **DropFields** transform to drop partition columns\n\n#### Read more about AWS Glue transforms here : https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data_clean = raw_data_x.drop_fields(['partition_0', 'partition_1', 'partition_2', 'partition_3'])",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Show sample transformed cleaned raw records\n- to.DF()method converts a DynamicFrame to an Apache Spark DataFrame by converting DynamicRecords into DataFrame fields\n- use show() method to display a sample of records in the frame\n- here were are showing the top 5 records in the DF\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data_clean.toDF().show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Redshift Connection Parameters\n- We will use \"**analytics_workshop**\" Glue connection to connect to Redshift cluster.\n- We will create connection option for raw table consisting of schema name, table name and database name.\n- We will create a temp output directory for Glue to use as a staging area for loading data into Redshift.\n- Make sure you change the S3 bucket name yourname-analytics-workshop-bucket to reflect your bucket name\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "connection_options_raw = {\n    \"dbtable\": \"redshift_lab.f_raw_1\",\n    \"database\": \"dev\"\n}\n\noutput_dir_tmp = \"s3://yourname-analytics-workshop-bucket/data\"",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Cast columns into desired format\n- We will explicitly cast all columns into desired datatypes.\n- If we dont perform this step, redshift on mismatch will create additional columns and then load the data. Ex: \"device_ts\" defined as timestamp in Redshift raw table DDL. If we dont cast this column from string to timestamp, a new column will be created in redshift \"f_raw_1\" table as \"device_ts_string\" which will have device_ts attribute values while original \"device_ts\" column which is defined as timestamp will stay blank.\n\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "raw_data_clean = ApplyMapping.apply(\n    frame=raw_data_clean,\n    mappings=[\n        (\"uuid\", \"string\", \"uuid\", \"string\"),\n        (\"device_ts\", \"string\", \"device_ts\", \"timestamp\"),\n        (\"device_id\", \"int\", \"device_id\", \"int\"),\n        (\"device_temp\", \"int\", \"device_temp\", \"int\"),\n        (\"track_id\", \"int\", \"track_id\", \"int\"),\n        (\"activity_type\", \"string\", \"activity_type\", \"string\"),\n        (\"load_time\", \"int\", \"load_time\", \"int\")\n    ]\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Load raw data in Redshift\n\n- Finally, we will load cleaned raw data dynamic frame into redshift table - \"**redshift_lab.f_raw_1**\"\n- We will Glue dynamic frame writer class to perform this action.\n\n#### Read more about AWS Glue dynamic frame writer here : https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame-writer.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "try:\n    print(\"INFO: Loading raw data into Amazon Redshift\")\n    glueContext.write_dynamic_frame.from_jdbc_conf(\n        frame=raw_data_clean,\n        catalog_connection=\"analytics_workshop\",\n        connection_options=connection_options_raw,\n        redshift_tmp_dir=output_dir_tmp + \"/tmp/\"\n    )\n    print(\"INFO: Raw data loading into Amazon Redshift complete\")\nexcept Exception as e:\n    print(f\"ERROR: An exception has occurred: {str(e)}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Redshift Connection Parameters\n- We will use \"**analytics_workshop**\" Glue connection to connect to Redshift cluster.\n- We will create connection option for raw table consisting of schema name, table name and database name.\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "connection_options_rd = {\n    \"dbtable\": \"redshift_lab.d_ref_data_1\",\n    \"database\": \"dev\"\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Cast columns into desired format\n- We will explicitly cast all columns into desired datatypes.\n- If we dont perform this step, redshift on mismatch will create additional columns and then load the data. Ex: \"track_id\" defined as integer in Redshift raw table DDL. If we dont cast this column from string to int, a new column will be created in redshift \"d_ref_data_1\" table as \"track_id_string\" which will have track_id attribute values while original \"track_id\" column which is defined as int will stay blank.\n\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "reference_data_clean = ApplyMapping.apply(\n    frame=reference_data,\n    mappings=[\n        (\"track_id\", \"string\", \"track_id\", \"int\"),\n        (\"track_name\", \"string\", \"track_name\", \"string\"),\n        (\"artist_name\", \"string\", \"artist_name\", \"string\")\n    ]\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Show sample transformed reference records\n- to.DF()method converts a DynamicFrame to an Apache Spark DataFrame by converting DynamicRecords into DataFrame fields\n- use show() method to display a sample of records in the frame\n- here were are showing the top 5 records in the DF\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "reference_data_clean.toDF().show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "## Load reference data in Redshift\n\n- Finally, we will load cleaned reference data dynamic frame into redshift table - \"**redshift_lab.d_ref_data_1**\"\n- We will Glue dynamic frame writer class to perform this action.\n\n#### Read more about AWS Glue dynamic frame writer here : https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame-writer.html\n\n#### Execute Code »",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "try:\n    print(\"INFO: Loading reference data into Amazon Redshift\")\n    glueContext.write_dynamic_frame.from_jdbc_conf(\n        frame=reference_data_clean,\n        catalog_connection = \"analytics_workshop\",\n        connection_options = connection_options_rd,\n        redshift_tmp_dir = output_dir_tmp + \"/tmp/\"\n    )\n    print(\"INFO: Reference data loading into Amazon Redshift complete\")\nexcept Exception as e:\n    print(f\"ERROR: An exception has occurred: {str(e)}\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Bonus Knowledge\n\n  - After you have finished developing your notebook, you can save the job and then run it. You can find the script in the **Script** tab. Any magics you added to the notebook will be stripped away and won't be saved as part of the script of the generated AWS Glue job. AWS Glue Studio will auto-add a job.commit() to the end of your generated script from the notebook contents.For more information about running jobs, see [Start a job run](https://docs.aws.amazon.com/glue/latest/ug/managing-jobs-chapter.html#start-jobs).\n  - You can schedule this job to run at hourly, daily, weekly, monthly or custom (cron expression) frequency under **Schedules** tab.\n  - You can integrate your job with Git version control systems such as AWS CodeCommit and GitHub. Read more about it [here](https://docs.aws.amazon.com/glue/latest/ug/edit-job-add-source-control-integration.html).",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# \n=========================",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "### If you wish you take this notebook and its output back home - you can download / export it using **Download Notebook** option.",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "# NEXT Steps: Go back to the lab guide",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "=========================",
			"metadata": {}
		}
	]
}